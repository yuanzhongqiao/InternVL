<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><div class="markdown-heading" dir="auto"><h1 tabindex="-1" class="heading-element" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenGVLab/InternVL/assets/8529570/5aa4cda8-b453-40a0-9336-17012b430ae8"><img width="60" alt="å›¾åƒ" src="https://github.com/OpenGVLab/InternVL/assets/8529570/5aa4cda8-b453-40a0-9336-17012b430ae8" style="max-width: 100%;"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVLç³»åˆ—ï¼šé€šè¿‡å¼€æºå¥—ä»¶ç¼©å°ä¸å•†ä¸šå¤šå¼è”è¿æ¨¡å‹çš„å·®è·â€”â€”GPT-4Vçš„å¼€åˆ›æ€§å¼€æºæ›¿ä»£å“</font></font></h1><a id="user-content--internvl-family-closing-the-gap-to-commercial-multimodal-models-with-open-source-suites--a-pioneering-open-source-alternative-to-gpt-4v" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šInternVL ç³»åˆ—ï¼šåˆ©ç”¨å¼€æºå¥—ä»¶ç¼©å°ä¸å•†ä¸šå¤šå¼è”è¿æ¨¡å‹çš„å·®è·â€”â€”GPT-4V çš„å¼€åˆ›æ€§å¼€æºæ›¿ä»£æ–¹æ¡ˆ" href="#-internvl-family-closing-the-gap-to-commercial-multimodal-models-with-open-source-suites--a-pioneering-open-source-alternative-to-gpt-4v"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[</font></font><a href="/OpenGVLab/InternVL/blob/main/BLOG.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ›´æ–°åšå®¢</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] [</font></font><a href="https://arxiv.org/abs/2312.14238" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è®ºæ–‡</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] [ </font></font><a href="https://arxiv.org/abs/2404.16821" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL 1.5 æŠ€æœ¯æŠ¥å‘Š</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] [</font></font><a href="https://internvl.opengvlab.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">èŠå¤©æ¼”ç¤º</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] </font></font><a href="https://huggingface.co/spaces/OpenGVLab/InternVL" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[HuggingFace æ¼”ç¤º ]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [</font></font><a href="#quick-start-with-huggingface"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¿«é€Ÿå…¥é—¨</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] [</font></font><a href="https://zhuanlan.zhihu.com/p/675877376" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸­æ–‡é˜…è¯»</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">]</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ–°é—»ğŸš€ğŸš€ğŸš€</font></font></h2><a id="user-content-news" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šæ–°é—»ğŸš€ğŸš€ğŸš€" href="#news"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>2024/04/28</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šæˆ‘ä»¬å‘å¸ƒäº† InternVL-Chat-V1-5 çš„ INT8 ç‰ˆæœ¬ï¼Œè¯·å‚è§</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ­¤å¤„</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚</font></font></li>
<li><code>2024/04/28</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šæˆ‘ä»¬åœ¨ Infographics VQA åŸºå‡†æµ‹è¯•ä¸­å®ç°äº† SOTA æ€§èƒ½ (75.74)ï¼Œè¯·å‚è§</font></font><a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=3" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ­¤å¤„</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚</font></font></li>
<li><code>2024/04/18</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šInternVL-Chat-V1.5å·²åœ¨</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HF link</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å‘å¸ƒï¼Œåœ¨MMMUã€DocVQAã€ChartQAã€MathVistaç­‰å„ç§åŸºå‡†æµ‹è¯•ä¸Šæ¥è¿‘GPT-4Vå’ŒGemini Proçš„æ€§èƒ½ã€‚</font></font></li>
<li><code>2024/02/27</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šInternVL è¢« CVPR 2024 æ¥å—ï¼ ğŸ‰</font></font></li>
<li><code>2024/02/24</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šInternVL-Chat æ¨¡å‹å·²åŒ…å«åœ¨</font></font><a href="https://github.com/open-compass/VLMEvalKit"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VLMEvalKit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸­ã€‚</font></font></li>
<li><code>2024/02/21</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼š</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL-Chat-V1.2-Plus</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">åœ¨ MathVista (59.9)ã€MMBench (83.8) å’Œ MMVP (58.7) ä¸Šå®ç°äº† SOTA æ€§èƒ½ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„</font></font><a href="/OpenGVLab/InternVL/blob/main/BLOG.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">åšå®¢</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</font></font></li>
<li><code>2024/02/12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: InternVL-Chat-V1.2 å·²å‘å¸ƒã€‚å®ƒåœ¨ MMMU val ä¸Šè¾¾åˆ° 51.6ï¼Œåœ¨ MMBench æµ‹è¯•ä¸Šè¾¾åˆ° 82.3ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„</font></font><a href="/OpenGVLab/InternVL/blob/main/BLOG.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">åšå®¢</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SFT æ•°æ®</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æˆ–å°è¯•æˆ‘ä»¬çš„</font></font><a href="https://internvl.opengvlab.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ¼”ç¤º</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚è¯¥æ¨¡å‹ç°å·²åœ¨</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HuggingFace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸Šæä¾›ï¼Œå¹¶ä¸”è®­ç»ƒ/è¯„ä¼°æ•°æ®å’Œè„šæœ¬éƒ½æ˜¯å¼€æºçš„ã€‚</font></font></li>
<li><code>2024/02/04</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼š</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL-Chat-V1.1åœ¨</font></font></a><font style="vertical-align: inherit;"></font><a href="https://github.com/tsb0601/MMVP"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MMVP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸Šè¾¾åˆ° 44.67% </font><font style="vertical-align: inherit;">ï¼Œé«˜äº GPT-4Vï¼</font></font></li>
<li><code>2024/01/27</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šæˆ‘ä»¬å‘å¸ƒäº†448åˆ†è¾¨ç‡æ¨¡å‹ï¼Œåœ¨MMBench devä¸Šè¾¾åˆ°äº†76.6ï¼Œå‚è§</font></font><a href="https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#-evaluation-chinese-models"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è¿™é‡Œ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚</font></font></li>
<li><code>2024/01/24</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: InternVL-Chat-V1.1 å‘å¸ƒï¼Œæ”¯æŒä¸­æ–‡ï¼ŒOCR èƒ½åŠ›æ›´å¼ºï¼Œè¯·çœ‹</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è¿™é‡Œ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æˆ–å°è¯•æˆ‘ä»¬çš„</font></font><a href="https://internvl.opengvlab.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ¼”ç¤º</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚</font></font></li>
<li><code>2024/01/16</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼šæˆ‘ä»¬å‘å¸ƒäº†ä¸ DeepSpeed é›†æˆçš„</font></font><a href="https://github.com/OpenGVLab/InternVL-MMDetSeg"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®šåˆ¶ mmcv/mmsegmentation/mmdetection ä»£ç </font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼Œå¯ç”¨äºè®­ç»ƒå¤§è§„æ¨¡å¯¹è±¡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</font></font></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ–‡ä»¶</font></font></h2><a id="user-content-documents" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šæ–‡æ¡£" href="#documents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚ä½•å®‰è£…InternVLï¼Ÿ</font></font><a href="/OpenGVLab/InternVL/blob/main/INSTALLATION.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[å…³è”]</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚ä½•å¾®è°ƒInternVLï¼Ÿ</font></font><a href="/OpenGVLab/InternVL/blob/main/internvl_chat/README.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[å…³è”]</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚ä½•è¯„ä»·InternVL-Chat-V1-5ï¼Ÿ</font></font><a href="/OpenGVLab/InternVL/blob/main/document/how_to_evaluate_internvl_chat_1_5.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[å…³è”]</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚ä½•ä½¿ç”¨ VLMEvalKit è¯„ä¼° InternVL-Chat-V1-5ï¼Ÿ ï¼ˆæ¨èï¼‰</font></font><a href="/OpenGVLab/InternVL/blob/main/document/how_to_evaluate_internvl_chat_1_5_using_vlmevalkit.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[é“¾æ¥]</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚ä½•éƒ¨ç½²æœ¬åœ°demoï¼Ÿ</font></font><a href="/OpenGVLab/InternVL/blob/main/document/how_to_deploy_a_local_demo.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[å…³è”]</font></font></a></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸ SOTA VLLM ç›¸æ¯”</font></font></h2><a id="user-content-compared-with-sota-vllms" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šä¸ SOTA VLLM ç›¸æ¯”" href="#compared-with-sota-vllms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/23737120/326237019-38e8a632-229c-4b20-b7e1-77299dfc6cee.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjYyMzcwMTktMzhlOGE2MzItMjI5Yy00YjIwLWI3ZTEtNzcyOTlkZmM2Y2VlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc5Y2ViZGRjMGQzYjM1NzE5NDdhMzRhMzE1MjkwOGIxNTNmNzBhZjI3ZjRlZGI0OWY3MDZhOTc1MWUzNjUzNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.PP2axrJoagSEd-0NO1Wc-RXuS_gyiH5vl6b1WGp78-w"><img width="500" alt="å›¾åƒ" src="https://private-user-images.githubusercontent.com/23737120/326237019-38e8a632-229c-4b20-b7e1-77299dfc6cee.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjYyMzcwMTktMzhlOGE2MzItMjI5Yy00YjIwLWI3ZTEtNzcyOTlkZmM2Y2VlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc5Y2ViZGRjMGQzYjM1NzE5NDdhMzRhMzE1MjkwOGIxNTNmNzBhZjI3ZjRlZGI0OWY3MDZhOTc1MWUzNjUzNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.PP2axrJoagSEd-0NO1Wc-RXuS_gyiH5vl6b1WGp78-w" style="max-width: 100%;"></a></p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/23737120/326092358-e9065a58-86fa-47ef-be9a-eb734532e73f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjYwOTIzNTgtZTkwNjVhNTgtODZmYS00N2VmLWJlOWEtZWI3MzQ1MzJlNzNmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiYzI4MzY3NzliNDY5Mzk4MjYzZjgxMjVjMzM3ZjYzZGQ0MWU0Y2EzNmYwMGZmZWQxNjI4YjA4ZjliZTQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.6YPHsRhTcU3bxcbhPK6qSJddHBovWsMkHWWVKZ42XVA"><img width="1229" alt="å›¾åƒ" src="https://private-user-images.githubusercontent.com/23737120/326092358-e9065a58-86fa-47ef-be9a-eb734532e73f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjYwOTIzNTgtZTkwNjVhNTgtODZmYS00N2VmLWJlOWEtZWI3MzQ1MzJlNzNmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiYzI4MzY3NzliNDY5Mzk4MjYzZjgxMjVjMzM3ZjYzZGQ0MWU0Y2EzNmYwMGZmZWQxNjI4YjA4ZjliZTQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.6YPHsRhTcU3bxcbhPK6qSJddHBovWsMkHWWVKZ42XVA" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/23737120/326576629-2b4f2978-36ea-4065-841d-3651c58955ed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjY1NzY2MjktMmI0ZjI5NzgtMzZlYS00MDY1LTg0MWQtMzY1MWM1ODk1NWVkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgxZjM1MzNjNjUyZjQ1Y2UwMTM2ZTdiM2MwN2Y1ZGM2YzM1MDlhNzE3YTc3YmM3MzlkODE1NDNkM2FlZDliMDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zag5m72fnWI-VzIZB2_BLDVBco29umed9vB9Gw0Qdeg"><img width="1229" alt="å›¾åƒ" src="https://private-user-images.githubusercontent.com/23737120/326576629-2b4f2978-36ea-4065-841d-3651c58955ed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yMzczNzEyMC8zMjY1NzY2MjktMmI0ZjI5NzgtMzZlYS00MDY1LTg0MWQtMzY1MWM1ODk1NWVkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgxZjM1MzNjNjUyZjQ1Y2UwMTM2ZTdiM2MwN2Y1ZGM2YzM1MDlhNzE3YTc3YmM3MzlkODE1NDNkM2FlZDliMDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zag5m72fnWI-VzIZB2_BLDVBco29umed9vB9Gw0Qdeg" style="max-width: 100%;"></a>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä»€ä¹ˆæ˜¯InternVLï¼Ÿ</font></font></h2><a id="user-content-what-is-internvl" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šä»€ä¹ˆæ˜¯ InternVLï¼Ÿ" href="#what-is-internvl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL å°† ViT å‚æ•°æ‰©å±•åˆ°</font></font><em><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6B å‚æ•°</font></font></strong></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ï¼Œå¹¶å°†å…¶ä¸ LLM å¯¹é½ã€‚</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ¨¡å‹åŠ¨ç‰©å›­</font></font></h2><a id="user-content-model-zoo" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šæ¨¡å‹åŠ¨ç‰©å›­" href="#model-zoo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è§†è§‰å¤§è¯­è¨€æ¨¡å‹</font></font></strong></p>
<table>
<thead>
<tr>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ¨¡å‹</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ—¥æœŸ</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸‹è½½</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ç¬”è®°</font></font></th>
</tr>
</thead>
<tbody>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’V1.5-Int8</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´4æœˆ28æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL-Chat-V1-5çš„INT8ç‰ˆæœ¬</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’V1.5</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´4æœˆ18æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ”¯æŒ4Kå›¾åƒï¼›è¶…å¼ºOCRï¼›åœ¨ MMMUã€DocVQAã€ChartQAã€MathVista ç­‰å„ç§åŸºå‡†ä¸Šæ¥è¿‘ GPT-4V å’Œ Gemini Pro çš„æ€§èƒ½ã€‚ï¼ˆğŸ”¥æ–°ï¼‰</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’V1.2âˆ’Plus</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´2æœˆ21æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SFTæ•°æ®æ›´å¤šæ›´å¼º</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’V1.2</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´2æœˆ11æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LLM å‡çº§è‡³ 34B</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’V1.1</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´1æœˆ24æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ”¯æŒä¸­æ–‡ï¼ŒOCRæ›´å¼ºå¤§</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’19Bâˆ’448px</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024.02.03</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B-448px" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">448åˆ†è¾¨ç‡</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’19B</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2023å¹´12æœˆ25æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è‹±è¯­å¤šæ¨¡æ€å¯¹è¯</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’èŠå¤©âˆ’13B</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2023å¹´12æœˆ25æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è‹±è¯­å¤šæ¨¡æ€å¯¹è¯</font></font></td>
</tr>
</tbody>
</table>
<p dir="auto"><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹</font></font></strong></p>
<table>
<thead>
<tr>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ¨¡å‹</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ—¥æœŸ</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸‹è½½</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ç¬”è®°</font></font></th>
</tr>
</thead>
<tbody>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸViTâˆ’6Bâˆ’448pxâˆ’V1.5</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´4æœˆ20æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡ï¼Œè¶…å¼ºOCRï¼ˆğŸ”¥æ–°ï¼‰</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸViTâˆ’6Bâˆ’448pxâˆ’V1.2</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´2æœˆ11æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">448åˆ†è¾¨ç‡</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸViTâˆ’6Bâˆ’448pxâˆ’V1.0</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2024å¹´1æœˆ30æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">448åˆ†è¾¨ç‡</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸViTâˆ’6Bâˆ’224px</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2023å¹´12æœˆ22æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternViT-6B-224px" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è§†è§‰åŸºç¡€æ¨¡å‹</font></font></td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å®ä¹ ç”ŸVLâˆ’14Bâˆ’224px</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2023å¹´12æœˆ22æ—¥</font></font></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ğŸ¤—</font></font><a href="https://huggingface.co/OpenGVLab/InternVL-14B-224px" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é«˜é¢‘é“¾æ¥</font></font></a></td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹</font></font></td>
</tr>
</tbody>
</table>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL èƒ½åšä»€ä¹ˆï¼Ÿ</font></font></h2><a id="user-content-what-can-internvl-do" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šInternVL å¯ä»¥åšä»€ä¹ˆï¼Ÿ" href="#what-can-internvl-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è§†è§‰æ„ŸçŸ¥ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</font></font></summary>
<ul dir="auto">
<li>
<p dir="auto">Linear-Probe Image Classification <a href="/OpenGVLab/InternVL/blob/main/classification#-evaluation">[see details]</a></p>
<p dir="auto">ViT-22B uses the private JFT-3B dataset.</p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">#param</th>
<th align="center">IN-1K</th>
<th align="center">IN-ReaL</th>
<th align="center">IN-V2</th>
<th align="center">IN-A</th>
<th align="center">IN-R</th>
<th align="center">IN-Sketch</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenCLIP-G</td>
<td align="center">1.8B</td>
<td align="center">86.2</td>
<td align="center">89.4</td>
<td align="center">77.2</td>
<td align="center">63.8</td>
<td align="center">87.8</td>
<td align="center">66.4</td>
</tr>
<tr>
<td>DINOv2-g</td>
<td align="center">1.1B</td>
<td align="center">86.5</td>
<td align="center">89.6</td>
<td align="center">78.4</td>
<td align="center">75.9</td>
<td align="center">78.8</td>
<td align="center">62.5</td>
</tr>
<tr>
<td>EVA-01-CLIP-g</td>
<td align="center">1.1B</td>
<td align="center">86.5</td>
<td align="center">89.3</td>
<td align="center">77.4</td>
<td align="center">70.5</td>
<td align="center">87.7</td>
<td align="center">63.1</td>
</tr>
<tr>
<td>MAWS-ViT-6.5B</td>
<td align="center">6.5B</td>
<td align="center">87.8</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td>ViT-22B*</td>
<td align="center">21.7B</td>
<td align="center">89.5</td>
<td align="center">90.9</td>
<td align="center">83.2</td>
<td align="center">83.8</td>
<td align="center">87.4</td>
<td align="center">âˆ’</td>
</tr>
<tr>
<td>InternViT-6B (ours)</td>
<td align="center">5.9B</td>
<td align="center">88.2</td>
<td align="center">90.4</td>
<td align="center">79.9</td>
<td align="center">77.5</td>
<td align="center">89.8</td>
<td align="center">69.1</td>
</tr>
</tbody>
</table>
</li>
<li>
<p dir="auto">Semantic Segmentation <a href="/OpenGVLab/InternVL/blob/main/segmentation#-evaluation">[see details]</a></p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">decoder</th>
<th align="center">#param (train/total)</th>
<th align="center">crop size</th>
<th>mIoU</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenCLIP-G (frozen)</td>
<td align="center">Linear</td>
<td align="center">0.3M / 1.8B</td>
<td align="center">512</td>
<td>39.3</td>
</tr>
<tr>
<td>ViT-22B (frozen)</td>
<td align="center">Linear</td>
<td align="center">0.9M / 21.7B</td>
<td align="center">504</td>
<td>34.6</td>
</tr>
<tr>
<td>InternViT-6B (frozen)</td>
<td align="center">Linear</td>
<td align="center">0.5M / 5.9B</td>
<td align="center">504</td>
<td>47.2 (+12.6)</td>
</tr>
<tr>
<td>ViT-22B (frozen)</td>
<td align="center">UperNet</td>
<td align="center">0.8B / 22.5B</td>
<td align="center">504</td>
<td>52.7</td>
</tr>
<tr>
<td>InternViT-6B (frozen)</td>
<td align="center">UperNet</td>
<td align="center">0.4B / 6.3B</td>
<td align="center">504</td>
<td>54.9 (+2.2)</td>
</tr>
<tr>
<td>ViT-22B</td>
<td align="center">UperNet</td>
<td align="center">22.5B / 22.5B</td>
<td align="center">504</td>
<td>55.3</td>
</tr>
<tr>
<td>InternViT-6B</td>
<td align="center">UperNet</td>
<td align="center">6.3B / 6.3B</td>
<td align="center">504</td>
<td>58.9 (+3.6)</td>
</tr>
</tbody>
</table>
</li>
<li>
<p dir="auto">Zero-Shot Image Classification <a href="/OpenGVLab/InternVL/blob/main/clip_benchmark#imagenet-variants-and-objectnet">[see details]</a></p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">IN-1K</th>
<th align="center">IN-A</th>
<th align="center">IN-R</th>
<th align="center">IN-V2</th>
<th align="center">IN-Sketch</th>
<th align="center">ObjectNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenCLIP-G</td>
<td align="center">80.1</td>
<td align="center">69.3</td>
<td align="center">92.1</td>
<td align="center">73.6</td>
<td align="center">68.9</td>
<td align="center">73.0</td>
</tr>
<tr>
<td>EVA-02-CLIP-E+</td>
<td align="center">82.0</td>
<td align="center">82.1</td>
<td align="center">94.5</td>
<td align="center">75.7</td>
<td align="center">71.6</td>
<td align="center">79.6</td>
</tr>
<tr>
<td>ViT-22B*</td>
<td align="center">85.9</td>
<td align="center">90.1</td>
<td align="center">96.0</td>
<td align="center">80.9</td>
<td align="center">âˆ’</td>
<td align="center">87.6</td>
</tr>
<tr>
<td>InternVL-C (ours)</td>
<td align="center">83.2</td>
<td align="center">83.8</td>
<td align="center">95.5</td>
<td align="center">77.3</td>
<td align="center">73.9</td>
<td align="center">80.6</td>
</tr>
</tbody>
</table>
</li>
<li>
<p dir="auto">Multilingual Zero-Shot Image Classification <a href="/OpenGVLab/InternVL/blob/main/clip_benchmark#multilingual-imagenet-1k">[see details]</a></p>
<p dir="auto">EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian</p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">IN-1K (EN)</th>
<th align="center">IN-1K (ZH)</th>
<th align="center">IN-1K (JP)</th>
<th align="center">IN-1K (AR)</th>
<th align="center">IN-1K (IT)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Taiyi-CLIP-ViT-H</td>
<td align="center">-</td>
<td align="center">54.4</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td>WuKong-ViT-L-G</td>
<td align="center">-</td>
<td align="center">57.5</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td>CN-CLIP-ViT-H</td>
<td align="center">-</td>
<td align="center">59.6</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td>AltCLIP-ViT-L</td>
<td align="center">74.5</td>
<td align="center">59.6</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td>EVA-02-CLIP-E+</td>
<td align="center">82.0</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">41.2</td>
</tr>
<tr>
<td>OpenCLIP-XLM-R-H</td>
<td align="center">77.0</td>
<td align="center">55.7</td>
<td align="center">53.1</td>
<td align="center">37.0</td>
<td align="center">56.8</td>
</tr>
<tr>
<td>InternVL-C (ours)</td>
<td align="center">83.2</td>
<td align="center">64.5</td>
<td align="center">61.5</td>
<td align="center">44.9</td>
<td align="center">65.7</td>
</tr>
</tbody>
</table>
</li>
<li>
<p dir="auto">Zero-Shot Video Classification [see details]</p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">#frame</th>
<th align="center">K400</th>
<th align="center">K600</th>
<th align="center">K700</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenCLIP-G</td>
<td align="center">1</td>
<td align="center">65.9</td>
<td align="center">66.1</td>
<td align="center">59.2</td>
</tr>
<tr>
<td>EVA-02-CLIP-E+</td>
<td align="center">1</td>
<td align="center">69.8</td>
<td align="center">69.3</td>
<td align="center">63.4</td>
</tr>
<tr>
<td>InternVL-C (ours)</td>
<td align="center">1</td>
<td align="center">71.0</td>
<td align="center">71.3</td>
<td align="center">65.7</td>
</tr>
<tr>
<td>ViCLIP</td>
<td align="center">8</td>
<td align="center">75.7</td>
<td align="center">73.5</td>
<td align="center">66.4</td>
</tr>
<tr>
<td>InternVL-C (ours)</td>
<td align="center">8</td>
<td align="center">79.4</td>
<td align="center">78.8</td>
<td align="center">71.5</td>
</tr>
</tbody>
</table>
</li>
</ul>
</details>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è·¨æ¨¡æ€æ£€ç´¢ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</font></font></summary>
<ul dir="auto">
<li>
<p dir="auto">English Zero-Shot Image-Text Retrieval <a href="/OpenGVLab/InternVL/blob/main/clip_benchmark#flickr30k--coco">[see details]</a></p>
<table>
  <tbody><tr align="center">
      <td rowspan="3" align="left"><b>model</b></td>
      <td colspan="6" align="center"><b>Flickr30K</b></td>
      <td colspan="6" align="center"><b>COCO</b></td>
      <td rowspan="3" align="center"><b>avg</b></td>
</tr>
   <tr align="center">
      <td colspan="3" align="center"><b>image-to-text</b></td>
      <td colspan="3" align="center"><b>text-to-image</b></td>
       <td colspan="3" align="center"><b>image-to-text</b></td>
      <td colspan="3" align="center"><b>text-to-image</b></td>
   </tr>
   <tr>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
   </tr>
<tr align="center">
      <td align="left">OpenCLIP-G</td>
      <td>92.9</td>
      <td>99.3</td>
      <td>99.8</td>
      <td>79.5</td>
      <td>95.0</td>
      <td>97.1</td>
      <td>67.3</td>
      <td>86.9</td>
      <td>92.6</td>
      <td>51.4</td>
      <td>74.9</td>
      <td>83.0</td>
      <td>85.0</td>
   </tr>
<tr align="center">
      <td align="left">EVA-02-CLIP-E+</td>
      <td>93.9</td>
      <td>99.4</td>
      <td>99.8</td>
      <td>78.8</td>
      <td>94.2</td>
      <td>96.8</td>
      <td>68.8</td>
      <td>87.8</td>
      <td>92.8</td>
      <td>51.1</td>
      <td>75.0</td>
      <td>82.7</td>
      <td>85.1</td>
   </tr>
  <tr align="center">
      <td align="left">EVA-CLIP-8B</td>
      <td>95.6</td>
      <td>99.6</td>
      <td>99.9</td>
      <td>80.8</td>
      <td>95.5</td>
      <td>97.6</td>
      <td>70.3</td>
      <td>89.3</td>
      <td>93.9</td>
      <td>53.0</td>
      <td>76.0</td>
      <td>83.4</td>
      <td>86.2</td>
   </tr>
<tr align="center">
      <td align="left">InternVL-C (ours)</td>
      <td>94.7</td>
      <td>99.6</td>
      <td>99.9</td>
      <td>81.7</td>
      <td>96.0</td>
      <td>98.2</td>
      <td>70.6</td>
      <td>89.0</td>
      <td>93.5</td>
      <td>54.1</td>
      <td>77.3</td>
      <td>84.6</td>
      <td>86.6</td>
   </tr>
<tr align="center">
      <td align="left">InternVL-G (ours)</td>
      <td>95.7</td>
      <td>99.7</td>
      <td>99.9</td>
      <td>85.0</td>
      <td>97.0</td>
      <td>98.6</td>
      <td>74.9</td>
      <td>91.3</td>
      <td>95.2</td>
      <td>58.6</td>
      <td>81.3</td>
      <td>88.0</td>
      <td>88.8</td>
   </tr>
</tbody></table>
</li>
<li>
<p dir="auto">Chinese Zero-Shot Image-Text Retrieval <a href="/OpenGVLab/InternVL/blob/main/clip_benchmark#flickr30k-cn--coco-cn">[see details]</a></p>
<table>
  <tbody><tr align="center">
      <td rowspan="3" align="left"><b>model</b></td>
      <td colspan="6" align="center"><b>Flickr30K-CN</b></td>
      <td colspan="6" align="center"><b>COCO-CN</b></td>
      <td rowspan="3" align="center"><b>avg</b></td>
</tr>
   <tr align="center">
      <td colspan="3" align="center"><b>image-to-text</b></td>
      <td colspan="3" align="center"><b>text-to-image</b></td>
       <td colspan="3" align="center"><b>image-to-text</b></td>
      <td colspan="3" align="center"><b>text-to-image</b></td>
   </tr>
   <tr>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
   </tr>
<tr align="center">
      <td align="left">CN-CLIP-ViT-H</td>
      <td>81.6</td>
      <td>97.5</td>
      <td>98.8</td>
      <td>71.2</td>
      <td>91.4</td>
      <td>95.5</td>
      <td>63.0</td>
      <td>86.6</td>
      <td>92.9</td>
      <td>69.2</td>
      <td>89.9</td>
      <td>96.1</td>
      <td>86.1</td>
   </tr>
<tr align="center">
      <td align="left">OpenCLIP-XLM-R-H</td>
      <td>86.1</td>
      <td>97.5</td>
      <td>99.2</td>
      <td>71.0</td>
      <td>90.5</td>
      <td>94.9</td>
      <td>70.0</td>
      <td>91.5</td>
      <td>97.0</td>
      <td>66.1</td>
      <td>90.8</td>
      <td>96.0</td>
      <td>87.6</td>
   </tr>
<tr align="center">
      <td align="left">InternVL-C (ours)</td>
      <td>90.3</td>
      <td>98.8</td>
      <td>99.7</td>
      <td>75.1</td>
      <td>92.9</td>
      <td>96.4</td>
      <td>68.8</td>
      <td>92.0</td>
      <td>96.7</td>
      <td>68.9</td>
      <td>91.9</td>
      <td>96.5</td>
      <td>89.0</td>
   </tr>
<tr align="center">
      <td align="left">InternVL-G (ours)</td>
      <td>92.9</td>
      <td>99.4</td>
      <td>99.8</td>
      <td>77.7</td>
      <td>94.8</td>
      <td>97.3</td>
      <td>71.4</td>
      <td>93.9</td>
      <td>97.7</td>
      <td>73.8</td>
      <td>94.4</td>
      <td>98.1</td>
      <td>90.9</td>
   </tr>
</tbody></table>
</li>
<li>
<p dir="auto">Multilingual Zero-Shot Image-Text Retrieval on XTD <a href="/OpenGVLab/InternVL/blob/main/clip_benchmark#xtd">[see details]</a></p>
<table>
<thead>
<tr>
<th>method</th>
<th align="center">EN</th>
<th align="center">ES</th>
<th align="center">FR</th>
<th align="center">ZH</th>
<th align="center">IT</th>
<th align="center">KO</th>
<th align="center">RU</th>
<th align="center">JP</th>
<th align="center">average</th>
</tr>
</thead>
<tbody>
<tr>
<td>AltCLIP</td>
<td align="center">95.4</td>
<td align="center">94.1</td>
<td align="center">92.9</td>
<td align="center">95.1</td>
<td align="center">94.2</td>
<td align="center">94.4</td>
<td align="center">91.8</td>
<td align="center">91.7</td>
<td align="center">93.7</td>
</tr>
<tr>
<td>OpenCLIP-XLM-R-H</td>
<td align="center">97.3</td>
<td align="center">96.1</td>
<td align="center">94.5</td>
<td align="center">94.7</td>
<td align="center">96.0</td>
<td align="center">90.2</td>
<td align="center">93.9</td>
<td align="center">94.0</td>
<td align="center">94.6</td>
</tr>
<tr>
<td>InternVL-C (ours)</td>
<td align="center">97.3</td>
<td align="center">95.7</td>
<td align="center">95.1</td>
<td align="center">95.6</td>
<td align="center">96.0</td>
<td align="center">92.2</td>
<td align="center">93.3</td>
<td align="center">95.5</td>
<td align="center">95.1</td>
</tr>
<tr>
<td>InternVL-G (ours)</td>
<td align="center">98.6</td>
<td align="center">97.7</td>
<td align="center">96.5</td>
<td align="center">96.7</td>
<td align="center">96.9</td>
<td align="center">95.1</td>
<td align="center">94.8</td>
<td align="center">96.1</td>
<td align="center">96.6</td>
</tr>
</tbody>
</table>
</li>
</ul>
</details>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¤šæ¨¡æ€å¯¹è¯ï¼ˆå‚è§â€œä¸ SOTA VLLM çš„æ¯”è¾ƒâ€ï¼‰</font></font></summary>
</details>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Huggingface å¿«é€Ÿå¯åŠ¨</font></font></h2><a id="user-content-quick-start-with-huggingface" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šHuggingface å¿«é€Ÿå…¥é—¨" href="#quick-start-with-huggingface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä½¿ç”¨ InternViT-6Bï¼ˆç‚¹å‡»å±•å¼€ï¼‰</font></font></summary>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoModel</span>, <span class="pl-v">CLIPImageProcessor</span>

<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModel</span>.<span class="pl-en">from_pretrained</span>(
    <span class="pl-s">'OpenGVLab/InternViT-6B-224px'</span>,
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>,
    <span class="pl-s1">low_cpu_mem_usage</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>).<span class="pl-en">cuda</span>().<span class="pl-en">eval</span>()

<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'./examples/image1.jpg'</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>)

<span class="pl-s1">image_processor</span> <span class="pl-c1">=</span> <span class="pl-v">CLIPImageProcessor</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s">'OpenGVLab/InternViT-6B-224px'</span>)

<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-en">image_processor</span>(<span class="pl-s1">images</span><span class="pl-c1">=</span><span class="pl-s1">image</span>, <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">'pt'</span>).<span class="pl-s1">pixel_values</span>
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">pixel_values</span>.<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()

<span class="pl-s1">outputs</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-s1">pixel_values</span>)</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor

model = AutoModel.from_pretrained(
    'OpenGVLab/InternViT-6B-224px',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image = Image.open('./examples/image1.jpg').convert('RGB')

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-224px')

pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

outputs = model(pixel_values)" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
</details>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä½¿ç”¨ InternVL-Cï¼ˆå¯¹æ¯”ï¼‰å’Œ InternVL-Gï¼ˆç”Ÿæˆï¼‰ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</font></font></summary>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoModel</span>, <span class="pl-v">CLIPImageProcessor</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoTokenizer</span>


<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModel</span>.<span class="pl-en">from_pretrained</span>(
    <span class="pl-s">'OpenGVLab/InternVL-14B-224px'</span>,
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>,
    <span class="pl-s1">low_cpu_mem_usage</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>).<span class="pl-en">cuda</span>().<span class="pl-en">eval</span>()

<span class="pl-s1">image_processor</span> <span class="pl-c1">=</span> <span class="pl-v">CLIPImageProcessor</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s">'OpenGVLab/InternVL-14B-224px'</span>)

<span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">AutoTokenizer</span>.<span class="pl-en">from_pretrained</span>(
    <span class="pl-s">'OpenGVLab/InternVL-14B-224px'</span>, <span class="pl-s1">use_fast</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">add_eos_token</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">tokenizer</span>.<span class="pl-s1">pad_token_id</span> <span class="pl-c1">=</span> <span class="pl-c1">0</span>  <span class="pl-c"># set pad_token_id to 0</span>

<span class="pl-s1">images</span> <span class="pl-c1">=</span> [
    <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'./examples/image1.jpg'</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>),
    <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'./examples/image2.jpg'</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>),
    <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'./examples/image3.jpg'</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>)
]
<span class="pl-s1">prefix</span> <span class="pl-c1">=</span> <span class="pl-s">'summarize:'</span>
<span class="pl-s1">texts</span> <span class="pl-c1">=</span> [
    <span class="pl-s1">prefix</span> <span class="pl-c1">+</span> <span class="pl-s">'a photo of a red panda'</span>,  <span class="pl-c"># English</span>
    <span class="pl-s1">prefix</span> <span class="pl-c1">+</span> <span class="pl-s">'ä¸€å¼ ç†ŠçŒ«çš„ç…§ç‰‡'</span>,  <span class="pl-c"># Chinese</span>
    <span class="pl-s1">prefix</span> <span class="pl-c1">+</span> <span class="pl-s">'äºŒåŒ¹ã®çŒ«ã®å†™çœŸ'</span>  <span class="pl-c"># Japanese</span>
]

<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-en">image_processor</span>(<span class="pl-s1">images</span><span class="pl-c1">=</span><span class="pl-s1">images</span>, <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">'pt'</span>).<span class="pl-s1">pixel_values</span>
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">pixel_values</span>.<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()
<span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(<span class="pl-s1">texts</span>, <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">'pt'</span>, <span class="pl-s1">max_length</span><span class="pl-c1">=</span><span class="pl-c1">80</span>,
                      <span class="pl-s1">truncation</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-s">'max_length'</span>).<span class="pl-s1">input_ids</span>.<span class="pl-en">cuda</span>()

<span class="pl-c"># InternVL-C</span>
<span class="pl-s1">logits_per_image</span>, <span class="pl-s1">logits_per_text</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(
    <span class="pl-s1">image</span><span class="pl-c1">=</span><span class="pl-s1">pixel_values</span>, <span class="pl-s1">text</span><span class="pl-c1">=</span><span class="pl-s1">input_ids</span>, <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'InternVL-C'</span>)
<span class="pl-s1">probs</span> <span class="pl-c1">=</span> <span class="pl-s1">logits_per_image</span>.<span class="pl-en">softmax</span>(<span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">-</span><span class="pl-c1">1</span>)
<span class="pl-c"># tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],</span>
<span class="pl-c">#         [2.2949e-02, 9.7656e-01, 5.9903e-06],</span>
<span class="pl-c">#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device='cuda:0',</span>
<span class="pl-c">#        dtype=torch.bfloat16, grad_fn=&lt;SoftmaxBackward0&gt;)</span>

<span class="pl-c"># InternVL-G</span>
<span class="pl-s1">logits_per_image</span>, <span class="pl-s1">logits_per_text</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(
    <span class="pl-s1">image</span><span class="pl-c1">=</span><span class="pl-s1">pixel_values</span>, <span class="pl-s1">text</span><span class="pl-c1">=</span><span class="pl-s1">input_ids</span>, <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'InternVL-G'</span>)
<span class="pl-s1">probs</span> <span class="pl-c1">=</span> <span class="pl-s1">logits_per_image</span>.<span class="pl-en">softmax</span>(<span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">-</span><span class="pl-c1">1</span>)
<span class="pl-c"># tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],</span>
<span class="pl-c">#         [8.6060e-03, 9.9219e-01, 2.8759e-06],</span>
<span class="pl-c">#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device='cuda:0',</span>
<span class="pl-c">#        dtype=torch.bfloat16, grad_fn=&lt;SoftmaxBackward0&gt;)</span>

<span class="pl-c"># please set add_eos_token to False for generation</span>
<span class="pl-s1">tokenizer</span>.<span class="pl-s1">add_eos_token</span> <span class="pl-c1">=</span> <span class="pl-c1">False</span>
<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'./examples/image1.jpg'</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>)
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-en">image_processor</span>(<span class="pl-s1">images</span><span class="pl-c1">=</span><span class="pl-s1">image</span>, <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">'pt'</span>).<span class="pl-s1">pixel_values</span>
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">pixel_values</span>.<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()

<span class="pl-s1">tokenized</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(<span class="pl-s">"English caption:"</span>, <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">'pt'</span>)
<span class="pl-s1">pred</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">generate</span>(
    <span class="pl-s1">pixel_values</span><span class="pl-c1">=</span><span class="pl-s1">pixel_values</span>,
    <span class="pl-s1">input_ids</span><span class="pl-c1">=</span><span class="pl-s1">tokenized</span>.<span class="pl-s1">input_ids</span>.<span class="pl-en">cuda</span>(),
    <span class="pl-s1">attention_mask</span><span class="pl-c1">=</span><span class="pl-s1">tokenized</span>.<span class="pl-s1">attention_mask</span>.<span class="pl-en">cuda</span>(),
    <span class="pl-s1">num_beams</span><span class="pl-c1">=</span><span class="pl-c1">5</span>,
    <span class="pl-s1">min_new_tokens</span><span class="pl-c1">=</span><span class="pl-c1">8</span>,
)
<span class="pl-s1">caption</span> <span class="pl-c1">=</span> <span class="pl-s1">tokenizer</span>.<span class="pl-en">decode</span>(<span class="pl-s1">pred</span>[<span class="pl-c1">0</span>].<span class="pl-en">cpu</span>(), <span class="pl-s1">skip_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">True</span>).<span class="pl-en">strip</span>()
<span class="pl-c"># English caption: a red panda sitting on top of a wooden platform</span></pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor
from transformers import AutoTokenizer


model = AutoModel.from_pretrained(
    'OpenGVLab/InternVL-14B-224px',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternVL-14B-224px')

tokenizer = AutoTokenizer.from_pretrained(
    'OpenGVLab/InternVL-14B-224px', use_fast=False, add_eos_token=True)
tokenizer.pad_token_id = 0  # set pad_token_id to 0

images = [
    Image.open('./examples/image1.jpg').convert('RGB'),
    Image.open('./examples/image2.jpg').convert('RGB'),
    Image.open('./examples/image3.jpg').convert('RGB')
]
prefix = 'summarize:'
texts = [
    prefix + 'a photo of a red panda',  # English
    prefix + 'ä¸€å¼ ç†ŠçŒ«çš„ç…§ç‰‡',  # Chinese
    prefix + 'äºŒåŒ¹ã®çŒ«ã®å†™çœŸ'  # Japanese
]

pixel_values = image_processor(images=images, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()
input_ids = tokenizer(texts, return_tensors='pt', max_length=80,
                      truncation=True, padding='max_length').input_ids.cuda()

# InternVL-C
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-C')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],
#         [2.2949e-02, 9.7656e-01, 5.9903e-06],
#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# InternVL-G
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-G')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],
#         [8.6060e-03, 9.9219e-01, 2.8759e-06],
#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# please set add_eos_token to False for generation
tokenizer.add_eos_token = False
image = Image.open('./examples/image1.jpg').convert('RGB')
pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

tokenized = tokenizer(&quot;English caption:&quot;, return_tensors='pt')
pred = model.generate(
    pixel_values=pixel_values,
    input_ids=tokenized.input_ids.cuda(),
    attention_mask=tokenized.attention_mask.cuda(),
    num_beams=5,
    min_new_tokens=8,
)
caption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()
# English caption: a red panda sitting on top of a wooden platform" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
</details>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä½¿ç”¨ InternVL-Chatï¼ˆç‚¹å‡»å±•å¼€ï¼‰</font></font></summary>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoTokenizer</span>, <span class="pl-v">AutoModel</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">import</span> <span class="pl-s1">torchvision</span>.<span class="pl-s1">transforms</span> <span class="pl-k">as</span> <span class="pl-v">T</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>

<span class="pl-k">from</span> <span class="pl-s1">torchvision</span>.<span class="pl-s1">transforms</span>.<span class="pl-s1">functional</span> <span class="pl-k">import</span> <span class="pl-v">InterpolationMode</span>


<span class="pl-v">IMAGENET_MEAN</span> <span class="pl-c1">=</span> (<span class="pl-c1">0.485</span>, <span class="pl-c1">0.456</span>, <span class="pl-c1">0.406</span>)
<span class="pl-v">IMAGENET_STD</span> <span class="pl-c1">=</span> (<span class="pl-c1">0.229</span>, <span class="pl-c1">0.224</span>, <span class="pl-c1">0.225</span>)


<span class="pl-k">def</span> <span class="pl-en">build_transform</span>(<span class="pl-s1">input_size</span>):
    <span class="pl-v">MEAN</span>, <span class="pl-v">STD</span> <span class="pl-c1">=</span> <span class="pl-v">IMAGENET_MEAN</span>, <span class="pl-v">IMAGENET_STD</span>
    <span class="pl-s1">transform</span> <span class="pl-c1">=</span> <span class="pl-v">T</span>.<span class="pl-v">Compose</span>([
        <span class="pl-v">T</span>.<span class="pl-v">Lambda</span>(<span class="pl-k">lambda</span> <span class="pl-s1">img</span>: <span class="pl-s1">img</span>.<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>) <span class="pl-k">if</span> <span class="pl-s1">img</span>.<span class="pl-s1">mode</span> <span class="pl-c1">!=</span> <span class="pl-s">'RGB'</span> <span class="pl-k">else</span> <span class="pl-s1">img</span>),
        <span class="pl-v">T</span>.<span class="pl-v">Resize</span>((<span class="pl-s1">input_size</span>, <span class="pl-s1">input_size</span>), <span class="pl-s1">interpolation</span><span class="pl-c1">=</span><span class="pl-v">InterpolationMode</span>.<span class="pl-v">BICUBIC</span>),
        <span class="pl-v">T</span>.<span class="pl-v">ToTensor</span>(),
        <span class="pl-v">T</span>.<span class="pl-v">Normalize</span>(<span class="pl-s1">mean</span><span class="pl-c1">=</span><span class="pl-v">MEAN</span>, <span class="pl-s1">std</span><span class="pl-c1">=</span><span class="pl-v">STD</span>)
    ])
    <span class="pl-k">return</span> <span class="pl-s1">transform</span>


<span class="pl-k">def</span> <span class="pl-en">find_closest_aspect_ratio</span>(<span class="pl-s1">aspect_ratio</span>, <span class="pl-s1">target_ratios</span>, <span class="pl-s1">width</span>, <span class="pl-s1">height</span>, <span class="pl-s1">image_size</span>):
    <span class="pl-s1">best_ratio_diff</span> <span class="pl-c1">=</span> <span class="pl-en">float</span>(<span class="pl-s">'inf'</span>)
    <span class="pl-s1">best_ratio</span> <span class="pl-c1">=</span> (<span class="pl-c1">1</span>, <span class="pl-c1">1</span>)
    <span class="pl-s1">area</span> <span class="pl-c1">=</span> <span class="pl-s1">width</span> <span class="pl-c1">*</span> <span class="pl-s1">height</span>
    <span class="pl-k">for</span> <span class="pl-s1">ratio</span> <span class="pl-c1">in</span> <span class="pl-s1">target_ratios</span>:
        <span class="pl-s1">target_aspect_ratio</span> <span class="pl-c1">=</span> <span class="pl-s1">ratio</span>[<span class="pl-c1">0</span>] <span class="pl-c1">/</span> <span class="pl-s1">ratio</span>[<span class="pl-c1">1</span>]
        <span class="pl-s1">ratio_diff</span> <span class="pl-c1">=</span> <span class="pl-en">abs</span>(<span class="pl-s1">aspect_ratio</span> <span class="pl-c1">-</span> <span class="pl-s1">target_aspect_ratio</span>)
        <span class="pl-k">if</span> <span class="pl-s1">ratio_diff</span> <span class="pl-c1">&lt;</span> <span class="pl-s1">best_ratio_diff</span>:
            <span class="pl-s1">best_ratio_diff</span> <span class="pl-c1">=</span> <span class="pl-s1">ratio_diff</span>
            <span class="pl-s1">best_ratio</span> <span class="pl-c1">=</span> <span class="pl-s1">ratio</span>
        <span class="pl-k">elif</span> <span class="pl-s1">ratio_diff</span> <span class="pl-c1">==</span> <span class="pl-s1">best_ratio_diff</span>:
            <span class="pl-k">if</span> <span class="pl-s1">area</span> <span class="pl-c1">&gt;</span> <span class="pl-c1">0.5</span> <span class="pl-c1">*</span> <span class="pl-s1">image_size</span> <span class="pl-c1">*</span> <span class="pl-s1">image_size</span> <span class="pl-c1">*</span> <span class="pl-s1">ratio</span>[<span class="pl-c1">0</span>] <span class="pl-c1">*</span> <span class="pl-s1">ratio</span>[<span class="pl-c1">1</span>]:
                <span class="pl-s1">best_ratio</span> <span class="pl-c1">=</span> <span class="pl-s1">ratio</span>
    <span class="pl-k">return</span> <span class="pl-s1">best_ratio</span>


<span class="pl-k">def</span> <span class="pl-en">dynamic_preprocess</span>(<span class="pl-s1">image</span>, <span class="pl-s1">min_num</span><span class="pl-c1">=</span><span class="pl-c1">1</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>, <span class="pl-s1">image_size</span><span class="pl-c1">=</span><span class="pl-c1">448</span>, <span class="pl-s1">use_thumbnail</span><span class="pl-c1">=</span><span class="pl-c1">False</span>):
    <span class="pl-s1">orig_width</span>, <span class="pl-s1">orig_height</span> <span class="pl-c1">=</span> <span class="pl-s1">image</span>.<span class="pl-s1">size</span>
    <span class="pl-s1">aspect_ratio</span> <span class="pl-c1">=</span> <span class="pl-s1">orig_width</span> <span class="pl-c1">/</span> <span class="pl-s1">orig_height</span>

    <span class="pl-c"># calculate the existing image aspect ratio</span>
    <span class="pl-s1">target_ratios</span> <span class="pl-c1">=</span> <span class="pl-en">set</span>(
        (<span class="pl-s1">i</span>, <span class="pl-s1">j</span>) <span class="pl-k">for</span> <span class="pl-s1">n</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">min_num</span>, <span class="pl-s1">max_num</span> <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-c1">1</span>, <span class="pl-s1">n</span> <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-k">for</span> <span class="pl-s1">j</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-c1">1</span>, <span class="pl-s1">n</span> <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-k">if</span>
        <span class="pl-s1">i</span> <span class="pl-c1">*</span> <span class="pl-s1">j</span> <span class="pl-c1">&lt;=</span> <span class="pl-s1">max_num</span> <span class="pl-c1">and</span> <span class="pl-s1">i</span> <span class="pl-c1">*</span> <span class="pl-s1">j</span> <span class="pl-c1">&gt;=</span> <span class="pl-s1">min_num</span>)
    <span class="pl-s1">target_ratios</span> <span class="pl-c1">=</span> <span class="pl-en">sorted</span>(<span class="pl-s1">target_ratios</span>, <span class="pl-s1">key</span><span class="pl-c1">=</span><span class="pl-k">lambda</span> <span class="pl-s1">x</span>: <span class="pl-s1">x</span>[<span class="pl-c1">0</span>] <span class="pl-c1">*</span> <span class="pl-s1">x</span>[<span class="pl-c1">1</span>])

    <span class="pl-c"># find the closest aspect ratio to the target</span>
    <span class="pl-s1">target_aspect_ratio</span> <span class="pl-c1">=</span> <span class="pl-en">find_closest_aspect_ratio</span>(
        <span class="pl-s1">aspect_ratio</span>, <span class="pl-s1">target_ratios</span>, <span class="pl-s1">orig_width</span>, <span class="pl-s1">orig_height</span>, <span class="pl-s1">image_size</span>)

    <span class="pl-c"># calculate the target width and height</span>
    <span class="pl-s1">target_width</span> <span class="pl-c1">=</span> <span class="pl-s1">image_size</span> <span class="pl-c1">*</span> <span class="pl-s1">target_aspect_ratio</span>[<span class="pl-c1">0</span>]
    <span class="pl-s1">target_height</span> <span class="pl-c1">=</span> <span class="pl-s1">image_size</span> <span class="pl-c1">*</span> <span class="pl-s1">target_aspect_ratio</span>[<span class="pl-c1">1</span>]
    <span class="pl-s1">blocks</span> <span class="pl-c1">=</span> <span class="pl-s1">target_aspect_ratio</span>[<span class="pl-c1">0</span>] <span class="pl-c1">*</span> <span class="pl-s1">target_aspect_ratio</span>[<span class="pl-c1">1</span>]

    <span class="pl-c"># resize the image</span>
    <span class="pl-s1">resized_img</span> <span class="pl-c1">=</span> <span class="pl-s1">image</span>.<span class="pl-en">resize</span>((<span class="pl-s1">target_width</span>, <span class="pl-s1">target_height</span>))
    <span class="pl-s1">processed_images</span> <span class="pl-c1">=</span> []
    <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">blocks</span>):
        <span class="pl-s1">box</span> <span class="pl-c1">=</span> (
            (<span class="pl-s1">i</span> <span class="pl-c1">%</span> (<span class="pl-s1">target_width</span> <span class="pl-c1">//</span> <span class="pl-s1">image_size</span>)) <span class="pl-c1">*</span> <span class="pl-s1">image_size</span>,
            (<span class="pl-s1">i</span> <span class="pl-c1">//</span> (<span class="pl-s1">target_width</span> <span class="pl-c1">//</span> <span class="pl-s1">image_size</span>)) <span class="pl-c1">*</span> <span class="pl-s1">image_size</span>,
            ((<span class="pl-s1">i</span> <span class="pl-c1">%</span> (<span class="pl-s1">target_width</span> <span class="pl-c1">//</span> <span class="pl-s1">image_size</span>)) <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-c1">*</span> <span class="pl-s1">image_size</span>,
            ((<span class="pl-s1">i</span> <span class="pl-c1">//</span> (<span class="pl-s1">target_width</span> <span class="pl-c1">//</span> <span class="pl-s1">image_size</span>)) <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-c1">*</span> <span class="pl-s1">image_size</span>
        )
        <span class="pl-c"># split the image</span>
        <span class="pl-s1">split_img</span> <span class="pl-c1">=</span> <span class="pl-s1">resized_img</span>.<span class="pl-en">crop</span>(<span class="pl-s1">box</span>)
        <span class="pl-s1">processed_images</span>.<span class="pl-en">append</span>(<span class="pl-s1">split_img</span>)
    <span class="pl-k">assert</span> <span class="pl-en">len</span>(<span class="pl-s1">processed_images</span>) <span class="pl-c1">==</span> <span class="pl-s1">blocks</span>
    <span class="pl-k">if</span> <span class="pl-s1">use_thumbnail</span> <span class="pl-c1">and</span> <span class="pl-en">len</span>(<span class="pl-s1">processed_images</span>) <span class="pl-c1">!=</span> <span class="pl-c1">1</span>:
        <span class="pl-s1">thumbnail_img</span> <span class="pl-c1">=</span> <span class="pl-s1">image</span>.<span class="pl-en">resize</span>((<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>))
        <span class="pl-s1">processed_images</span>.<span class="pl-en">append</span>(<span class="pl-s1">thumbnail_img</span>)
    <span class="pl-k">return</span> <span class="pl-s1">processed_images</span>


<span class="pl-k">def</span> <span class="pl-en">load_image</span>(<span class="pl-s1">image_file</span>, <span class="pl-s1">input_size</span><span class="pl-c1">=</span><span class="pl-c1">448</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>):
    <span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s1">image_file</span>).<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>)
    <span class="pl-s1">transform</span> <span class="pl-c1">=</span> <span class="pl-en">build_transform</span>(<span class="pl-s1">input_size</span><span class="pl-c1">=</span><span class="pl-s1">input_size</span>)
    <span class="pl-s1">images</span> <span class="pl-c1">=</span> <span class="pl-en">dynamic_preprocess</span>(<span class="pl-s1">image</span>, <span class="pl-s1">image_size</span><span class="pl-c1">=</span><span class="pl-s1">input_size</span>, <span class="pl-s1">use_thumbnail</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-s1">max_num</span>)
    <span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> [<span class="pl-en">transform</span>(<span class="pl-s1">image</span>) <span class="pl-k">for</span> <span class="pl-s1">image</span> <span class="pl-c1">in</span> <span class="pl-s1">images</span>]
    <span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">stack</span>(<span class="pl-s1">pixel_values</span>)
    <span class="pl-k">return</span> <span class="pl-s1">pixel_values</span>


<span class="pl-s1">path</span> <span class="pl-c1">=</span> <span class="pl-s">"OpenGVLab/InternVL-Chat-V1-5"</span>
<span class="pl-c"># If you have an 80G A100 GPU, you can put the entire model on a single GPU.</span>
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModel</span>.<span class="pl-en">from_pretrained</span>(
    <span class="pl-s1">path</span>,
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>,
    <span class="pl-s1">low_cpu_mem_usage</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>).<span class="pl-en">eval</span>().<span class="pl-en">cuda</span>()
<span class="pl-c"># Otherwise, you need to set device_map='auto' to use multiple GPUs for inference.</span>
<span class="pl-c"># model = AutoModel.from_pretrained(</span>
<span class="pl-c">#     path,</span>
<span class="pl-c">#     torch_dtype=torch.bfloat16,</span>
<span class="pl-c">#     low_cpu_mem_usage=True,</span>
<span class="pl-c">#     trust_remote_code=True,</span>
<span class="pl-c">#     device_map='auto').eval()</span>

<span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">AutoTokenizer</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s1">path</span>, <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-c"># set the max number of tiles in `max_num`</span>
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-en">load_image</span>(<span class="pl-s">'./examples/image1.jpg'</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()

<span class="pl-s1">generation_config</span> <span class="pl-c1">=</span> <span class="pl-en">dict</span>(
    <span class="pl-s1">num_beams</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,
    <span class="pl-s1">max_new_tokens</span><span class="pl-c1">=</span><span class="pl-c1">512</span>,
    <span class="pl-s1">do_sample</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,
)

<span class="pl-c"># single-round single-image conversation</span>
<span class="pl-s1">question</span> <span class="pl-c1">=</span> <span class="pl-s">"è¯·è¯¦ç»†æè¿°å›¾ç‰‡"</span> <span class="pl-c"># Please describe the picture in detail</span>
<span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>, <span class="pl-s1">question</span>, <span class="pl-s1">generation_config</span>)
<span class="pl-en">print</span>(<span class="pl-s1">question</span>, <span class="pl-s1">response</span>)

<span class="pl-c"># multi-round single-image conversation</span>
<span class="pl-s1">question</span> <span class="pl-c1">=</span> <span class="pl-s">"è¯·è¯¦ç»†æè¿°å›¾ç‰‡"</span> <span class="pl-c"># Please describe the picture in detail</span>
<span class="pl-s1">response</span>, <span class="pl-s1">history</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>, <span class="pl-s1">question</span>, <span class="pl-s1">generation_config</span>, <span class="pl-s1">history</span><span class="pl-c1">=</span><span class="pl-c1">None</span>, <span class="pl-s1">return_history</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-en">print</span>(<span class="pl-s1">question</span>, <span class="pl-s1">response</span>)

<span class="pl-s1">question</span> <span class="pl-c1">=</span> <span class="pl-s">"è¯·æ ¹æ®å›¾ç‰‡å†™ä¸€é¦–è¯—"</span> <span class="pl-c"># Please write a poem according to the picture</span>
<span class="pl-s1">response</span>, <span class="pl-s1">history</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>, <span class="pl-s1">question</span>, <span class="pl-s1">generation_config</span>, <span class="pl-s1">history</span><span class="pl-c1">=</span><span class="pl-s1">history</span>, <span class="pl-s1">return_history</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-en">print</span>(<span class="pl-s1">question</span>, <span class="pl-s1">response</span>)

<span class="pl-c"># multi-round multi-image conversation</span>
<span class="pl-s1">pixel_values1</span> <span class="pl-c1">=</span> <span class="pl-en">load_image</span>(<span class="pl-s">'./examples/image1.jpg'</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()
<span class="pl-s1">pixel_values2</span> <span class="pl-c1">=</span> <span class="pl-en">load_image</span>(<span class="pl-s">'./examples/image2.jpg'</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">cat</span>((<span class="pl-s1">pixel_values1</span>, <span class="pl-s1">pixel_values2</span>), <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">0</span>)

<span class="pl-s1">question</span> <span class="pl-c1">=</span> <span class="pl-s">"è¯¦ç»†æè¿°è¿™ä¸¤å¼ å›¾ç‰‡"</span> <span class="pl-c"># Describe the two pictures in detail</span>
<span class="pl-s1">response</span>, <span class="pl-s1">history</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>, <span class="pl-s1">question</span>, <span class="pl-s1">generation_config</span>, <span class="pl-s1">history</span><span class="pl-c1">=</span><span class="pl-c1">None</span>, <span class="pl-s1">return_history</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-en">print</span>(<span class="pl-s1">question</span>, <span class="pl-s1">response</span>)

<span class="pl-s1">question</span> <span class="pl-c1">=</span> <span class="pl-s">"è¿™ä¸¤å¼ å›¾ç‰‡çš„ç›¸åŒç‚¹å’ŒåŒºåˆ«åˆ†åˆ«æ˜¯ä»€ä¹ˆ"</span> <span class="pl-c"># What are the similarities and differences between these two pictures</span>
<span class="pl-s1">response</span>, <span class="pl-s1">history</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>, <span class="pl-s1">question</span>, <span class="pl-s1">generation_config</span>, <span class="pl-s1">history</span><span class="pl-c1">=</span><span class="pl-s1">history</span>, <span class="pl-s1">return_history</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-en">print</span>(<span class="pl-s1">question</span>, <span class="pl-s1">response</span>)

<span class="pl-c"># batch inference (single image per sample)</span>
<span class="pl-s1">pixel_values1</span> <span class="pl-c1">=</span> <span class="pl-en">load_image</span>(<span class="pl-s">'./examples/image1.jpg'</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()
<span class="pl-s1">pixel_values2</span> <span class="pl-c1">=</span> <span class="pl-en">load_image</span>(<span class="pl-s">'./examples/image2.jpg'</span>, <span class="pl-s1">max_num</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-en">to</span>(<span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>).<span class="pl-en">cuda</span>()
<span class="pl-s1">image_counts</span> <span class="pl-c1">=</span> [<span class="pl-s1">pixel_values1</span>.<span class="pl-en">size</span>(<span class="pl-c1">0</span>), <span class="pl-s1">pixel_values2</span>.<span class="pl-en">size</span>(<span class="pl-c1">0</span>)]
<span class="pl-s1">pixel_values</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">cat</span>((<span class="pl-s1">pixel_values1</span>, <span class="pl-s1">pixel_values2</span>), <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">0</span>)

<span class="pl-s1">questions</span> <span class="pl-c1">=</span> [<span class="pl-s">"Describe the image in detail."</span>] <span class="pl-c1">*</span> <span class="pl-en">len</span>(<span class="pl-s1">image_counts</span>)
<span class="pl-s1">responses</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">batch_chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">pixel_values</span>,
                             <span class="pl-s1">image_counts</span><span class="pl-c1">=</span><span class="pl-s1">image_counts</span>,
                             <span class="pl-s1">questions</span><span class="pl-c1">=</span><span class="pl-s1">questions</span>,
                             <span class="pl-s1">generation_config</span><span class="pl-c1">=</span><span class="pl-s1">generation_config</span>)
<span class="pl-k">for</span> <span class="pl-s1">question</span>, <span class="pl-s1">response</span> <span class="pl-c1">in</span> <span class="pl-en">zip</span>(<span class="pl-s1">questions</span>, <span class="pl-s1">responses</span>):
    <span class="pl-en">print</span>(<span class="pl-s1">question</span>)
    <span class="pl-en">print</span>(<span class="pl-s1">response</span>)</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="from transformers import AutoTokenizer, AutoModel
import torch
import torchvision.transforms as T
from PIL import Image

from torchvision.transforms.functional import InterpolationMode


IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)


def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform


def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio


def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images


def load_image(image_file, input_size=448, max_num=6):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values


path = &quot;OpenGVLab/InternVL-Chat-V1-5&quot;
# If you have an 80G A100 GPU, you can put the entire model on a single GPU.
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).eval().cuda()
# Otherwise, you need to set device_map='auto' to use multiple GPUs for inference.
# model = AutoModel.from_pretrained(
#     path,
#     torch_dtype=torch.bfloat16,
#     low_cpu_mem_usage=True,
#     trust_remote_code=True,
#     device_map='auto').eval()

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
# set the max number of tiles in `max_num`
pixel_values = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()

generation_config = dict(
    num_beams=1,
    max_new_tokens=512,
    do_sample=False,
)

# single-round single-image conversation
question = &quot;è¯·è¯¦ç»†æè¿°å›¾ç‰‡&quot; # Please describe the picture in detail
response = model.chat(tokenizer, pixel_values, question, generation_config)
print(question, response)

# multi-round single-image conversation
question = &quot;è¯·è¯¦ç»†æè¿°å›¾ç‰‡&quot; # Please describe the picture in detail
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)
print(question, response)

question = &quot;è¯·æ ¹æ®å›¾ç‰‡å†™ä¸€é¦–è¯—&quot; # Please write a poem according to the picture
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)
print(question, response)

# multi-round multi-image conversation
pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

question = &quot;è¯¦ç»†æè¿°è¿™ä¸¤å¼ å›¾ç‰‡&quot; # Describe the two pictures in detail
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)
print(question, response)

question = &quot;è¿™ä¸¤å¼ å›¾ç‰‡çš„ç›¸åŒç‚¹å’ŒåŒºåˆ«åˆ†åˆ«æ˜¯ä»€ä¹ˆ&quot; # What are the similarities and differences between these two pictures
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)
print(question, response)

# batch inference (single image per sample)
pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()
image_counts = [pixel_values1.size(0), pixel_values2.size(0)]
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

questions = [&quot;Describe the image in detail.&quot;] * len(image_counts)
responses = model.batch_chat(tokenizer, pixel_values,
                             image_counts=image_counts,
                             questions=questions,
                             generation_config=generation_config)
for question, response in zip(questions, responses):
    print(question)
    print(response)" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
</details>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ‰§ç…§</font></font></h2><a id="user-content-license" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šè®¸å¯è¯" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è¯¥é¡¹ç›®æ˜¯åœ¨</font></font><a href="/OpenGVLab/InternVL/blob/main/LICENSE"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MIT è®¸å¯</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ä¸‹å‘å¸ƒçš„ã€‚è¯¥é¡¹ç›®çš„éƒ¨åˆ†å†…å®¹åŒ…å«æ¥è‡ªå…¶ä»–æ¥æºçš„ä»£ç å’Œæ¨¡å‹ï¼Œè¿™äº›ä»£ç å’Œæ¨¡å‹å—å„è‡ªçš„è®¸å¯çº¦æŸã€‚</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¼•æ–‡</font></font></h2><a id="user-content-citation" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šå¼•æ–‡" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚æœæ‚¨å‘ç°è¯¥é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰ç”¨ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š</font></font></p>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">@article</span>{<span class="pl-en">chen2023internvl</span>,
  <span class="pl-s">title</span>=<span class="pl-s"><span class="pl-pds">{</span>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>=<span class="pl-s"><span class="pl-pds">{</span>Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>=<span class="pl-s"><span class="pl-pds">{</span>arXiv preprint arXiv:2312.14238<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>=<span class="pl-s"><span class="pl-pds">{</span>2023<span class="pl-pds">}</span></span>
}

<span class="pl-k">@article</span>{<span class="pl-en">chen2024far</span>,
  <span class="pl-s">title</span>=<span class="pl-s"><span class="pl-pds">{</span>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>=<span class="pl-s"><span class="pl-pds">{</span>Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>=<span class="pl-s"><span class="pl-pds">{</span>arXiv preprint arXiv:2404.16821<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>=<span class="pl-s"><span class="pl-pds">{</span>2024<span class="pl-pds">}</span></span>
}</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">è‡´è°¢</font></font></h2><a id="user-content-acknowledgement" class="anchor" aria-label="æ°¸ä¹…é“¾æ¥ï¼šè‡´è°¢" href="#acknowledgement"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternVL å‚è€ƒä»¥ä¸‹é¡¹ç›®çš„ä»£ç æ„å»ºï¼š</font></font><a href="https://github.com/openai/CLIP"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI CLIP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/mlfoundations/open_clip"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Open CLIP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/LAION-AI/CLIP_benchmark"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CLIP Benchmark</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/baaivision/EVA/tree/master"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">EVA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/OpenGVLab/InternImage"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InternImage</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/czczup/ViT-Adapter"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ViT-Adapter</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/open-mmlab/mmsegmentation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MMSegmentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/huggingface/transformers"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/facebookresearch/dinov2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DINOv2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BLIP-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€</font></font><a href="https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Qwen-VL</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å’Œ</font></font><a href="https://github.com/haotian-liu/LLaVA"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LLaVA-1.5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€‚æ„Ÿè°¢ä»–ä»¬å‡ºè‰²çš„å·¥ä½œï¼</font></font></p>
<hr>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">å¦‚æœæ‚¨æƒ³åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤ï¼Œè¯·æ‰«æä»¥ä¸‹äºŒç»´ç æ·»åŠ æˆ‘ä»¬çš„åŠ©æ‰‹ä¸ºå¾®ä¿¡å¥½å‹ï¼š</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/26198430/253845075-e3f0807f-956a-474e-8fd2-1f7c22d73997.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yNjE5ODQzMC8yNTM4NDUwNzUtZTNmMDgwN2YtOTU2YS00NzRlLThmZDItMWY3YzIyZDczOTk3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiYzNlNjM5NzYxMDEwZjgxMjljOTFiNGFhNWQ5NWU5YTg3MWM1ZjNjNzNkOWE0MDYxNTAzOWM0ZWZiNmY5YjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.aGtTXv1samBjLKY7J4cRQMG7wkH70DmX2csR15HwGZY"><img width="300" alt="å›¾åƒ" src="https://private-user-images.githubusercontent.com/26198430/253845075-e3f0807f-956a-474e-8fd2-1f7c22d73997.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ2MzMyNjksIm5iZiI6MTcxNDYzMjk2OSwicGF0aCI6Ii8yNjE5ODQzMC8yNTM4NDUwNzUtZTNmMDgwN2YtOTU2YS00NzRlLThmZDItMWY3YzIyZDczOTk3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAyVDA2NTYwOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiYzNlNjM5NzYxMDEwZjgxMjljOTFiNGFhNWQ5NWU5YTg3MWM1ZjNjNzNkOWE0MDYxNTAzOWM0ZWZiNmY5YjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.aGtTXv1samBjLKY7J4cRQMG7wkH70DmX2csR15HwGZY" style="max-width: 100%;"></a></p>
</article></div>
